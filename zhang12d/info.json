{
    "abstract": "Optimizing multivariate performance measure is an important task in Machine Learning.  Joachims (2005) introduced a Support Vector Method whose underlying optimization problem is commonly solved by cutting plane methods (CPMs) such as SVM-Perf and BMRM.  It can be shown that CPMs converge to an <i>&#949;</i> accurate solution in <i>O(1/&#955; &#949;)</i> iterations, where <i>&#955;</i> is the trade-off parameter between the regularizer and the loss function.  Motivated by the impressive convergence rate of CPM on a number of practical problems, it was conjectured that these rates can be further improved.  We disprove this conjecture in this paper by constructing counter examples.  However, surprisingly, we further discover that these problems are not inherently hard, and we develop a novel smoothing strategy, which in conjunction with Nesterov's accelerated gradient method, can find an <i>&#949;</i> accurate solution in <i>O<sup>*</sup> (min {1/&#949;, 1/&#8730;&#955;&#949;})</i> iterations.  Computationally, our smoothing technique is also particularly advantageous for optimizing multivariate performance scores such as precision/recall break-even point and ROCArea; the cost per iteration remains the same as that of CPMs.  Empirical evaluation on some of the largest publicly available data sets shows that our method converges significantly faster than CPMs without sacrificing generalization ability.",
    "authors": [
        "Xinhua Zhang",
        "Ankan Saha",
        "S.V.N. Vishwanathan"
    ],
    "id": "zhang12d",
    "title": "Smoothing Multivariate Performance Measures",
    "volume": "13",
    "year": "2012"
}