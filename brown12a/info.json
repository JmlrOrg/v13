{
    "abstract": "We present a unifying framework for information theoretic feature selection, bringing almost two decades of research on heuristic filter criteria under a single theoretical interpretation.  This is in response to the question: <i>\"what are the implicit statistical assumptions of feature selection criteria based on mutual information?\"</i>.  To answer this, we adopt a different strategy than is usual in the feature selection literature&minus;instead of trying to <i>define</i> a criterion, we <i>derive</i> one, directly from a clearly specified objective function: the conditional likelihood of the training labels.  While many hand-designed heuristic criteria try to optimize a definition of feature 'relevancy' and 'redundancy', our approach leads to a probabilistic framework which naturally incorporates these concepts.  As a result we can unify the numerous criteria published over the last two decades, and show them to be low-order approximations to the exact (but intractable) optimisation problem.  The primary contribution is to show that <i>common heuristics for information based feature selection (including Markov Blanket algorithms as a special case) are approximate iterative maximisers of the conditional likelihood.</i> A large empirical study provides strong evidence to favour certain classes of criteria, in particular those that balance the relative size of the relevancy/redundancy terms.  Overall we conclude that the JMI criterion (Yang and Moody, 1999; Meyer et al., 2008) provides the best tradeoff in terms of accuracy, stability, and flexibility with small data samples.",
    "authors": [
        "Gavin Brown",
        "Adam Pocock",
        "Ming-Jie Zhao",
        "Mikel Luj{{\\'a}}n"
    ],
    "id": "brown12a",
    "issue": 1,
    "pages": [
        27,
        66
    ],
    "title": "Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection",
    "volume": "13",
    "year": "2012"
}